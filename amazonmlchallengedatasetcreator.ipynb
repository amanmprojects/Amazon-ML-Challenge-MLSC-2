{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9405976,"sourceType":"datasetVersion","datasetId":5710734}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from datasets import Dataset","metadata":{"execution":{"iopub.status.busy":"2024-09-16T11:51:12.913638Z","iopub.execute_input":"2024-09-16T11:51:12.914746Z","iopub.status.idle":"2024-09-16T11:51:12.921369Z","shell.execute_reply.started":"2024-09-16T11:51:12.914684Z","shell.execute_reply":"2024-09-16T11:51:12.919739Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ntrain = pd.read_csv(\"/kaggle/input/traincsv/train.csv\")\n# train = pd.read_csv(\"train.csv\")\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2024-09-16T11:51:12.923519Z","iopub.execute_input":"2024-09-16T11:51:12.923915Z","iopub.status.idle":"2024-09-16T11:51:13.649647Z","shell.execute_reply.started":"2024-09-16T11:51:12.923874Z","shell.execute_reply":"2024-09-16T11:51:13.648300Z"},"trusted":true},"execution_count":62,"outputs":[{"execution_count":62,"output_type":"execute_result","data":{"text/plain":"                                          image_link  group_id  entity_name  \\\n0  https://m.media-amazon.com/images/I/61I9XdN6OF...    748919  item_weight   \n1  https://m.media-amazon.com/images/I/71gSRbyXmo...    916768  item_volume   \n2  https://m.media-amazon.com/images/I/61BZ4zrjZX...    459516  item_weight   \n3  https://m.media-amazon.com/images/I/612mrlqiI4...    459516  item_weight   \n4  https://m.media-amazon.com/images/I/617Tl40LOX...    731432  item_weight   \n\n     entity_value  \n0      500.0 gram  \n1         1.0 cup  \n2      0.709 gram  \n3      0.709 gram  \n4  1400 milligram  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_link</th>\n      <th>group_id</th>\n      <th>entity_name</th>\n      <th>entity_value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>https://m.media-amazon.com/images/I/61I9XdN6OF...</td>\n      <td>748919</td>\n      <td>item_weight</td>\n      <td>500.0 gram</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>https://m.media-amazon.com/images/I/71gSRbyXmo...</td>\n      <td>916768</td>\n      <td>item_volume</td>\n      <td>1.0 cup</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>https://m.media-amazon.com/images/I/61BZ4zrjZX...</td>\n      <td>459516</td>\n      <td>item_weight</td>\n      <td>0.709 gram</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>https://m.media-amazon.com/images/I/612mrlqiI4...</td>\n      <td>459516</td>\n      <td>item_weight</td>\n      <td>0.709 gram</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>https://m.media-amazon.com/images/I/617Tl40LOX...</td>\n      <td>731432</td>\n      <td>item_weight</td>\n      <td>1400 milligram</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2024-09-16T11:51:13.651112Z","iopub.execute_input":"2024-09-16T11:51:13.651625Z","iopub.status.idle":"2024-09-16T11:51:13.710177Z","shell.execute_reply.started":"2024-09-16T11:51:13.651577Z","shell.execute_reply":"2024-09-16T11:51:13.708830Z"},"trusted":true},"execution_count":63,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 263859 entries, 0 to 263858\nData columns (total 4 columns):\n #   Column        Non-Null Count   Dtype \n---  ------        --------------   ----- \n 0   image_link    263859 non-null  object\n 1   group_id      263859 non-null  int64 \n 2   entity_name   263859 non-null  object\n 3   entity_value  263859 non-null  object\ndtypes: int64(1), object(3)\nmemory usage: 8.1+ MB\n","output_type":"stream"}]},{"cell_type":"markdown","source":"##### Shuffling and getting the subset","metadata":{}},{"cell_type":"code","source":"# Shuffle the DataFrame\nshuffled_train = train.sample(frac=1, random_state=42).reset_index(drop=True)\n\n# Ensure the first 5000 rows contain all types of values of entity_name\nsubset_size = 10000\nunique_entity_names = shuffled_train['entity_name'].unique()\nrequired_rows = []\n\nfor entity in unique_entity_names:\n    entity_rows = shuffled_train[shuffled_train['entity_name'] == entity]\n    required_rows.append(entity_rows)\n\n# Concatenate the required rows and shuffle again to mix them\nrequired_rows_df = pd.concat(required_rows).sample(frac=1, random_state=42).reset_index(drop=True)\n\n# Ensure all rows with the same group_id are included\ngroup_ids = required_rows_df['group_id'].unique()\nfinal_rows = []\n\nfor group_id in group_ids:\n    group_rows = shuffled_train[shuffled_train['group_id'] == group_id]\n    final_rows.append(group_rows)\n\n# Concatenate the final rows and sort by group_id\nfinal_subset_df = pd.concat(final_rows).sort_values(by='group_id').reset_index(drop=True)\n\n# Select the first 5000 rows\nfinal_subset = final_subset_df.head(subset_size)\n\n# Update the train DataFrame to be the final subset\ntrain = final_subset\ntrain.to_csv(\"train_subset.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T11:51:13.713407Z","iopub.execute_input":"2024-09-16T11:51:13.713970Z","iopub.status.idle":"2024-09-16T11:51:15.035085Z","shell.execute_reply.started":"2024-09-16T11:51:13.713911Z","shell.execute_reply":"2024-09-16T11:51:15.033933Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"markdown","source":"##### Function to download images","metadata":{}},{"cell_type":"code","source":"import os\nimport multiprocessing\nfrom functools import partial\nfrom tqdm import tqdm\nfrom PIL import Image\nfrom pathlib import Path\nimport urllib.request\nimport time\n\ndef create_placeholder_image(image_save_path):\n    try:\n        placeholder_image = Image.new('RGB', (100, 100), color='black')\n        placeholder_image.save(image_save_path)\n    except Exception as e:\n        return\n\ndef download_image(image_link, save_folder, retries=3, delay=3):\n    if not isinstance(image_link, str):\n        return\n\n    filename = Path(image_link).name\n    image_save_path = os.path.join(save_folder, filename)\n\n    if os.path.exists(image_save_path):\n        return\n\n    for _ in range(retries):\n        try:\n#             print(\"Downloading image: \", image_link)\n            urllib.request.urlretrieve(image_link, image_save_path)\n            return image_save_path\n        except:\n            time.sleep(delay)\n    \n    create_placeholder_image(image_save_path) #Create a black placeholder image for invalid links/images\n\ndef download_images(image_links, download_folder, allow_multiprocessing=True):\n    if not os.path.exists(download_folder):\n        os.makedirs(download_folder)\n\n    if allow_multiprocessing:\n        download_image_partial = partial(\n            download_image, save_folder=download_folder, retries=3, delay=3)\n\n        with multiprocessing.Pool(64) as pool:\n            list(tqdm(pool.imap(download_image_partial, image_links), total=len(image_links)))\n            print('.....\\n')\n            pool.close()\n            pool.join()\n    else:\n        for image_link in tqdm(image_links, total=len(image_links)):\n            download_image(image_link, save_folder=download_folder, retries=3, delay=3)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T11:51:15.036701Z","iopub.execute_input":"2024-09-16T11:51:15.037184Z","iopub.status.idle":"2024-09-16T11:51:15.050779Z","shell.execute_reply.started":"2024-09-16T11:51:15.037128Z","shell.execute_reply":"2024-09-16T11:51:15.049297Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"cwd = os.getcwd()\nimages_dir = os.path.join(cwd, 'images')\nif not os.path.exists(images_dir):\n    os.makedirs(images_dir)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T11:51:15.052388Z","iopub.execute_input":"2024-09-16T11:51:15.052797Z","iopub.status.idle":"2024-09-16T11:51:15.069105Z","shell.execute_reply.started":"2024-09-16T11:51:15.052755Z","shell.execute_reply":"2024-09-16T11:51:15.067861Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"markdown","source":"##### Download Images","metadata":{}},{"cell_type":"code","source":"image_links = train[\"image_link\"]\ndownload_images(image_links, \"images\")","metadata":{"execution":{"iopub.status.busy":"2024-09-16T11:51:15.070618Z","iopub.execute_input":"2024-09-16T11:51:15.071016Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"  0%|          | 0/10000 [00:00<?, ?it/s]","output_type":"stream"}]},{"cell_type":"markdown","source":"##### Create the python object in one go (might not work due to memory constraints)","metadata":{}},{"cell_type":"code","source":"from PIL import Image\nfrom matplotlib import pyplot\ndata_dict = {}\n\ndef get_image_path(image_link, kaggle=False):\n    if kaggle is True:\n        return f\"kaggle/working/images/{Path(image_link).name}\"\n    else:\n        return f\"images/{Path(image_link).name}\"\n\ntest_image_path = get_image_path(train['image_link'][0])\nprint(test_image_path)\nif(os.path.exists(test_image_path)):\n    print (True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for index, row in train.iterrows():\n    data_dict[index] = {\n        \"image_link\": row[\"image_link\"],\n        \"entity_name\": row[\"entity_name\"],\n        \"entity_value\": row[\"entity_value\"],\n        \"image_path\": get_image_path(row[\"image_link\"])\n    }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Load as HF dataset","metadata":{}},{"cell_type":"code","source":"import tarfile\n\nwith tarfile.open('images.tar.gz', 'w:gz') as tar:\n    tar.add('./images')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = Dataset.from_dict(data_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### HF login and pushing the dataset to HF","metadata":{}},{"cell_type":"code","source":"import getpass\nos.environ[\"HF_TOKEN\"] = getpass.getpass(\"Enter your Hugging Face token: \")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login\n\nlogin(token=os.environ[\"HF_TOKEN\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install huggingface_hub -q","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import HfApi\n\n# Instantiate the API\napi = HfApi()\n\n\n\n# Create the dataset repository if it doesn't exist\napi.create_repo(repo_id=\"amanm10000/amazon-ml-challenge-train\", repo_type=\"dataset\", exist_ok=True, token=os.environ['HF_TOKEN'])\n\n# Upload the file to the dataset\napi.upload_file(\n    path_or_fileobj=\"/kaggle/working/images.tar.gz\",\n    path_in_repo=\"images.tar.gz\",\n    repo_id=\"amanm10000/amazon-ml-challenge-train\",\n    repo_type=\"dataset\",\n    token=os.environ['HF_TOKEN']\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}