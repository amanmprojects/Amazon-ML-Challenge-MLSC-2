{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_link</th>\n",
       "      <th>group_id</th>\n",
       "      <th>entity_name</th>\n",
       "      <th>entity_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://m.media-amazon.com/images/I/61I9XdN6OF...</td>\n",
       "      <td>748919</td>\n",
       "      <td>item_weight</td>\n",
       "      <td>500.0 gram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://m.media-amazon.com/images/I/71gSRbyXmo...</td>\n",
       "      <td>916768</td>\n",
       "      <td>item_volume</td>\n",
       "      <td>1.0 cup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://m.media-amazon.com/images/I/61BZ4zrjZX...</td>\n",
       "      <td>459516</td>\n",
       "      <td>item_weight</td>\n",
       "      <td>0.709 gram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://m.media-amazon.com/images/I/612mrlqiI4...</td>\n",
       "      <td>459516</td>\n",
       "      <td>item_weight</td>\n",
       "      <td>0.709 gram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://m.media-amazon.com/images/I/617Tl40LOX...</td>\n",
       "      <td>731432</td>\n",
       "      <td>item_weight</td>\n",
       "      <td>1400 milligram</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image_link  group_id  entity_name  \\\n",
       "0  https://m.media-amazon.com/images/I/61I9XdN6OF...    748919  item_weight   \n",
       "1  https://m.media-amazon.com/images/I/71gSRbyXmo...    916768  item_volume   \n",
       "2  https://m.media-amazon.com/images/I/61BZ4zrjZX...    459516  item_weight   \n",
       "3  https://m.media-amazon.com/images/I/612mrlqiI4...    459516  item_weight   \n",
       "4  https://m.media-amazon.com/images/I/617Tl40LOX...    731432  item_weight   \n",
       "\n",
       "     entity_value  \n",
       "0      500.0 gram  \n",
       "1         1.0 cup  \n",
       "2      0.709 gram  \n",
       "3      0.709 gram  \n",
       "4  1400 milligram  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# train = pd.read_csv(\"/kaggle/input/traincsv/train.csv\")\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Shuffling and getting the subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the DataFrame\n",
    "shuffled_train = train.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Ensure the first 5000 rows contain all types of values of entity_name\n",
    "subset_size = 5000\n",
    "unique_entity_names = shuffled_train['entity_name'].unique()\n",
    "required_rows = []\n",
    "\n",
    "for entity in unique_entity_names:\n",
    "    entity_rows = shuffled_train[shuffled_train['entity_name'] == entity]\n",
    "    required_rows.append(entity_rows)\n",
    "\n",
    "# Concatenate the required rows and shuffle again to mix them\n",
    "required_rows_df = pd.concat(required_rows).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Ensure all rows with the same group_id are included\n",
    "group_ids = required_rows_df['group_id'].unique()\n",
    "final_rows = []\n",
    "\n",
    "for group_id in group_ids:\n",
    "    group_rows = shuffled_train[shuffled_train['group_id'] == group_id]\n",
    "    final_rows.append(group_rows)\n",
    "\n",
    "# Concatenate the final rows and sort by group_id\n",
    "final_subset_df = pd.concat(final_rows).sort_values(by='group_id').reset_index(drop=True)\n",
    "\n",
    "# Select the first 5000 rows\n",
    "final_subset = final_subset_df.head(subset_size)\n",
    "\n",
    "# Update the train DataFrame to be the final subset\n",
    "train = final_subset\n",
    "train.to_csv(\"train_subset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function to download images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import urllib.request\n",
    "import time\n",
    "\n",
    "def create_placeholder_image(image_save_path):\n",
    "    try:\n",
    "        placeholder_image = Image.new('RGB', (100, 100), color='black')\n",
    "        placeholder_image.save(image_save_path)\n",
    "    except Exception as e:\n",
    "        return\n",
    "\n",
    "def download_image(image_link, save_folder, retries=3, delay=3):\n",
    "    if not isinstance(image_link, str):\n",
    "        return\n",
    "\n",
    "    filename = Path(image_link).name\n",
    "    image_save_path = os.path.join(save_folder, filename)\n",
    "\n",
    "    if os.path.exists(image_save_path):\n",
    "        return\n",
    "\n",
    "    for _ in range(retries):\n",
    "        try:\n",
    "            print(\"Downloading image: \", image_link)\n",
    "            urllib.request.urlretrieve(image_link, image_save_path)\n",
    "            return image_save_path\n",
    "        except:\n",
    "            time.sleep(delay)\n",
    "    \n",
    "    create_placeholder_image(image_save_path) #Create a black placeholder image for invalid links/images\n",
    "\n",
    "def download_images(image_links, download_folder, allow_multiprocessing=True):\n",
    "    if not os.path.exists(download_folder):\n",
    "        os.makedirs(download_folder)\n",
    "\n",
    "    if allow_multiprocessing:\n",
    "        download_image_partial = partial(\n",
    "            download_image, save_folder=download_folder, retries=3, delay=3)\n",
    "\n",
    "        with multiprocessing.Pool(64) as pool:\n",
    "            list(tqdm(pool.imap(download_image_partial, image_links), total=len(image_links)))\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "    else:\n",
    "        for image_link in tqdm(image_links, total=len(image_links)):\n",
    "            download_image(image_link, save_folder=download_folder, retries=3, delay=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "images_dir = os.path.join(cwd, 'images')\n",
    "if not os.path.exists(images_dir):\n",
    "    os.makedirs(images_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Download Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_links = train[\"image_link\"].tolist()\n",
    "download_images(image_links, \"images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create the python object in one go (might not work due to memory constraints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {}\n",
    "\n",
    "def get_image_path(image_link):\n",
    "    return f\"images/{Path(image_link).name}\"\n",
    "\n",
    "for index, row in train.iterrows():\n",
    "    data_dict[index] = {\n",
    "        \"group_id\": row[\"group_id\"],\n",
    "        \"image\": Image.open(get_image_path(row[\"image_link\"])),\n",
    "        \"image_link\": row[\"image_link\"],\n",
    "        \"entity_name\": row[\"entity_name\"],\n",
    "        \"entity_value\": row[\"entity_value\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "\n",
    "def is_compatible_with_pyarrow(data_dict):\n",
    "    try:\n",
    "        # Attempt to convert the data_dict to a PyArrow Table\n",
    "        pa.Table.from_pydict(data_dict)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Data is not compatible with PyArrow: {e}\")\n",
    "        return False\n",
    "\n",
    "# Check compatibility\n",
    "if is_compatible_with_pyarrow(data_dict):\n",
    "    print(\"data_dict is compatible with PyArrow.\")\n",
    "else:\n",
    "    print(\"data_dict is not compatible with PyArrow.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert the images to byte arrays and ensure all values are strings to make the data compatible with PyArrow\n",
    "import io\n",
    "\n",
    "def convert_image_to_bytes(image):\n",
    "    with io.BytesIO() as output:\n",
    "        image.save(output, format=\"PNG\")\n",
    "        return output.getvalue()\n",
    "\n",
    "for index in data_dict:\n",
    "    data_dict[index][\"image\"] = convert_image_to_bytes(data_dict[index][\"image\"])\n",
    "    data_dict[index][\"group_id\"] = str(data_dict[index][\"group_id\"])\n",
    "    data_dict[index][\"entity_value\"] = str(data_dict[index][\"entity_value\"])\n",
    "\n",
    "# Re-check compatibility with PyArrow\n",
    "if is_compatible_with_pyarrow(data_dict):\n",
    "    print(\"data_dict is now compatible with PyArrow.\")\n",
    "else:\n",
    "    print(\"data_dict is still not compatible with PyArrow.\")\n",
    "\n",
    "# Ensure all values in data_dict are strings to make the data compatible with PyArrow\n",
    "for index in data_dict:\n",
    "    for key in data_dict[index]:\n",
    "        if not isinstance(data_dict[index][key], bytes):\n",
    "            data_dict[index][key] = str(data_dict[index][key])\n",
    "\n",
    "# Re-check compatibility with PyArrow\n",
    "if is_compatible_with_pyarrow(data_dict):\n",
    "    print(\"data_dict is now compatible with PyArrow.\")\n",
    "else:\n",
    "    print(\"data_dict is still not compatible with PyArrow.\")\n",
    "    \n",
    "# Ensure all values in data_dict are either strings or bytes to make the data compatible with PyArrow\n",
    "for index in data_dict:\n",
    "    for key in data_dict[index]:\n",
    "        if not isinstance(data_dict[index][key], (bytes, str)):\n",
    "            data_dict[index][key] = str(data_dict[index][key])\n",
    "\n",
    "# Re-check compatibility with PyArrow\n",
    "if is_compatible_with_pyarrow(data_dict):\n",
    "    print(\"data_dict is now compatible with PyArrow.\")\n",
    "else:\n",
    "    print(\"data_dict is still not compatible with PyArrow.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load as HF dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_dict(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### HF login and pushing the dataset to HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "os.environ[\"HF_TOKEN\"] = getpass.getpass(\"Enter your Hugging Face token: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=os.environ[\"HF_TOKEN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.push_to_hub(\"amanm10000/amazon-ml-challenge-train\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
