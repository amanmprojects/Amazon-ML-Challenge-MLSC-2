{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('../dataset/train.csv')\n",
    "df = df.iloc[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# set cuda_launch_blocking to True in os\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amanm/Desktop/MLSC/AmazonMLChallenge/student_resource 3/myenv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VisionEncoderDecoderModel(\n",
       "  (encoder): DonutSwinModel(\n",
       "    (embeddings): DonutSwinEmbeddings(\n",
       "      (patch_embeddings): DonutSwinPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "      )\n",
       "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): DonutSwinEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): DonutSwinStage(\n",
       "          (blocks): ModuleList(\n",
       "            (0-1): 2 x DonutSwinLayer(\n",
       "              (layernorm_before): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): DonutSwinAttention(\n",
       "                (self): DonutSwinSelfAttention(\n",
       "                  (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "                  (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "                  (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): DonutSwinSelfOutput(\n",
       "                  (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DonutSwinDropPath(p=0.1)\n",
       "              (layernorm_after): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): DonutSwinIntermediate(\n",
       "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): DonutSwinOutput(\n",
       "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): DonutSwinPatchMerging(\n",
       "            (reduction): Linear(in_features=512, out_features=256, bias=False)\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): DonutSwinStage(\n",
       "          (blocks): ModuleList(\n",
       "            (0-1): 2 x DonutSwinLayer(\n",
       "              (layernorm_before): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): DonutSwinAttention(\n",
       "                (self): DonutSwinSelfAttention(\n",
       "                  (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): DonutSwinSelfOutput(\n",
       "                  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DonutSwinDropPath(p=0.1)\n",
       "              (layernorm_after): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): DonutSwinIntermediate(\n",
       "                (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): DonutSwinOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): DonutSwinPatchMerging(\n",
       "            (reduction): Linear(in_features=1024, out_features=512, bias=False)\n",
       "            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (2): DonutSwinStage(\n",
       "          (blocks): ModuleList(\n",
       "            (0-13): 14 x DonutSwinLayer(\n",
       "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): DonutSwinAttention(\n",
       "                (self): DonutSwinSelfAttention(\n",
       "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): DonutSwinSelfOutput(\n",
       "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DonutSwinDropPath(p=0.1)\n",
       "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): DonutSwinIntermediate(\n",
       "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): DonutSwinOutput(\n",
       "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): DonutSwinPatchMerging(\n",
       "            (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "            (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (3): DonutSwinStage(\n",
       "          (blocks): ModuleList(\n",
       "            (0-1): 2 x DonutSwinLayer(\n",
       "              (layernorm_before): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): DonutSwinAttention(\n",
       "                (self): DonutSwinSelfAttention(\n",
       "                  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): DonutSwinSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DonutSwinDropPath(p=0.1)\n",
       "              (layernorm_after): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): DonutSwinIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): DonutSwinOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): AdaptiveAvgPool1d(output_size=1)\n",
       "  )\n",
       "  (decoder): MBartForCausalLM(\n",
       "    (model): MBartDecoderWrapper(\n",
       "      (decoder): MBartDecoder(\n",
       "        (embed_tokens): MBartScaledWordEmbedding(57532, 1024, padding_idx=1)\n",
       "        (embed_positions): MBartLearnedPositionalEmbedding(130, 1024)\n",
       "        (layers): ModuleList(\n",
       "          (0-3): 4 x MBartDecoderLayer(\n",
       "            (self_attn): MBartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_fn): GELUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MBartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (lm_head): Linear(in_features=1024, out_features=57532, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
    "import torch\n",
    "\n",
    "processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base-finetuned-docvqa\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base-finetuned-docvqa\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_links = df['image_link']\n",
    "entity_names = df['entity_name']\n",
    "\n",
    "# get image name from image link\n",
    "def get_image_name(image_link):\n",
    "    return image_link.split('/')[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# def get_pixel_values(df, range_of_images):\n",
    "#     df = df.iloc[range_of_images:range_of_images+100]\n",
    "#     image_names = df['image_link'].apply(get_image_name)\n",
    "#     image_paths = [os.path.join('../images', image_name) for image_name in image_names]\n",
    "#     images = []\n",
    "#     for image_path in image_paths:\n",
    "#         if os.path.exists(image_path):\n",
    "#             # Use PIL to open the image to check its format\n",
    "#             try:\n",
    "#                 img = Image.open(image_path)\n",
    "#                 # Check if the image is a placeholder (black 100x100)\n",
    "#                 if img.size == (100, 100) and img.getpixel((0, 0)) == (0, 0, 0):\n",
    "#                     print(f\"Skipping placeholder image: {image_path}\")\n",
    "#                     continue\n",
    "#                 # Convert to RGB format if necessary\n",
    "#                 if img.mode != 'RGB':\n",
    "#                     img = img.convert('RGB')\n",
    "#                 # Convert to numpy array for OpenCV\n",
    "#                 img = np.array(img)\n",
    "#                 # Convert color space for OpenCV\n",
    "#                 img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "#                 images.append(img)\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Failed to process image: {image_path}, Error: {e}\")\n",
    "#         else:\n",
    "#             print(f\"Image file not found: {image_path}\")\n",
    "#     pixel_values = processor(images=images, return_tensors=\"pt\").pixel_values\n",
    "#     return pixel_values\n",
    "\n",
    "\n",
    "# # prepare decoder input ids\n",
    "# def get_decoder_input_ids(df, range_of_images):\n",
    "#     df = df.iloc[range_of_images:range_of_images+100]\n",
    "#     entity_names = df['entity_name'].tolist()\n",
    "#     task_prompt = \"What is the {entity_name}?\"\n",
    "#     prompts = [task_prompt.replace(\"{entity_name}\", entity_name) for entity_name in entity_names]\n",
    "#     decoder_input_ids = processor.tokenizer(prompts, add_special_tokens=False, return_tensors=\"pt\").input_ids\n",
    "#     return decoder_input_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "\n",
    "\n",
    "# for i in range(0, len(df), 20):\n",
    "#     pixel_values = get_pixel_values(df, range_of_images=i)\n",
    "#     decoder_input_ids = get_decoder_input_ids(df, range_of_images=i)\n",
    "    \n",
    "#     outputs = model.generate(\n",
    "#         pixel_values.to(device),\n",
    "#         decoder_input_ids=decoder_input_ids.to(device),\n",
    "#         max_length=model.decoder.config.max_position_embeddings,\n",
    "#         early_stopping=True,\n",
    "#         pad_token_id=processor.tokenizer.pad_token_id,\n",
    "#         eos_token_id=processor.tokenizer.eos_token_id,\n",
    "#         use_cache=True,\n",
    "#         num_beams=1,\n",
    "#         bad_words_ids=[[processor.tokenizer.unk_token_id]],\n",
    "#         return_dict_in_generate=True\n",
    "#     )\n",
    "    \n",
    "#     sequences = processor.batch_decode(outputs.sequences)\n",
    "#     predicted_values = []\n",
    "    \n",
    "#     for seq in sequences:\n",
    "#         sequence = seq.replace(processor.tokenizer.eos_token, \"\").replace(processor.tokenizer.pad_token, \"\")\n",
    "#         sequence = re.sub(r\"<.*?>\", \"\", sequence, count=1).strip()  # remove first task start token\n",
    "#         predicted_values.append(processor.token2json(sequence))\n",
    "    \n",
    "#     predictions.loc[i:i+len(predicted_values)-1, 'predicted_entity_value'] = predicted_values\n",
    "    \n",
    "#     # Clear GPU memory\n",
    "#     del pixel_values, decoder_input_ids, outputs, sequences, predicted_values\n",
    "#     torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to process image: ../images/61I9XdN6OFL.jpg, Error: list index out of range\n",
      "Failed to process image: ../images/71gSRbyXmoL.jpg, Error: list index out of range\n",
      "Failed to process image: ../images/61BZ4zrjZXL.jpg, Error: list index out of range\n",
      "Failed to process image: ../images/612mrlqiI4L.jpg, Error: list index out of range\n",
      "Failed to process image: ../images/617Tl40LOXL.jpg, Error: list index out of range\n",
      "Failed to process image: ../images/61QsBSE7jgL.jpg, Error: list index out of range\n",
      "Failed to process image: ../images/81xsq6vf2qL.jpg, Error: list index out of range\n",
      "Failed to process image: ../images/71DiLRHeZdL.jpg, Error: list index out of range\n",
      "Failed to process image: ../images/91Cma3RzseL.jpg, Error: list index out of range\n",
      "Failed to process image: ../images/71jBLhmTNlL.jpg, Error: list index out of range\n",
      "Failed to process image: ../images/81N73b5khVL.jpg, Error: list index out of range\n",
      "Failed to process image: ../images/61oMj2iXOuL.jpg, Error: list index out of range\n",
      "Failed to process image: ../images/91LPf6OjV9L.jpg, Error: list index out of range\n",
      "Failed to process image: ../images/81fOxWWWKYL.jpg, Error: list index out of range\n",
      "Failed to process image: ../images/81dzao1Ob4L.jpg, Error: list index out of range\n",
      "Failed to process image: ../images/91-iahVGEDL.jpg, Error: list index out of range\n",
      "Failed to process image: ../images/81S2+GnYpTL.jpg, Error: list index out of range\n",
      "Failed to process image: ../images/81e2YtCOKvL.jpg, Error: list index out of range\n",
      "Failed to process image: ../images/81RNsNEM1EL.jpg, Error: list index out of range\n",
      "Failed to process image: ../images/91prZeizZnL.jpg, Error: list index out of range\n",
      "Failed to process image: ../images/31EvJszFVfL.jpg, Error: list index out of range\n",
      "Failed to process image: ../images/61wzlucTREL.jpg, Error: list index out of range\n",
      "Failed to process image: ../images/61sQ+qAKr4L.jpg, Error: list index out of range\n",
      "Failed to process image: ../images/81x77l2T5NL.jpg, Error: list index out of range\n",
      "Failed to process image: ../images/71nywfWZUwL.jpg, Error: list index out of range\n",
      "Failed to process image: ../images/71nywfWZUwL.jpg, Error: list index out of range\n",
      "Failed to process image: ../images/51WsuKKAVrL.jpg, Error: list index out of range\n",
      "Failed to process image: ../images/61XGDKap+JL.jpg, Error: list index out of range\n",
      "Failed to process image: ../images/715vVcWJxGL.jpg, Error: list index out of range\n",
      "Failed to process image: ../images/613v+2W4UwL.jpg, Error: list index out of range\n",
      "Failed to process image: ../images/71+fn9TWQmL.jpg, Error: list index out of range\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import cv2\n",
    "# Create a new DataFrame with all columns from df and add a new column 'predicted_entity_value'\n",
    "predicted_df = df.copy()\n",
    "predicted_df['predicted_entity_value'] = None\n",
    "\n",
    "\n",
    "for image_link in image_links:\n",
    "    image_name = get_image_name(image_link)\n",
    "    image_path = os.path.join('../images', image_name)\n",
    "    \n",
    "    if os.path.exists(image_path):\n",
    "        try:\n",
    "            img = Image.open(image_path)\n",
    "            # Check if the image is a placeholder (black 100x100)\n",
    "            if img.size == (100, 100) and img.getpixel((0, 0)) == (0, 0, 0):\n",
    "                print(f\"Skipping placeholder image: {image_path}\")\n",
    "                continue\n",
    "            # Convert to RGB format if necessary\n",
    "            if img.mode != 'RGB':\n",
    "                img = img.convert('RGB')\n",
    "            # Convert to numpy array for OpenCV\n",
    "            img = np.array(img)\n",
    "            # Convert color space for OpenCV\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "            pixel_values = processor(images=img, return_tensors=\"pt\").pixel_values\n",
    "            decoder_input_ids = processor.tokenizer([], add_special_tokens=False, return_tensors=\"pt\").input_ids\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process image: {image_path}, Error: {e}\")\n",
    "            continue\n",
    "    else:\n",
    "        print(f\"Image file not found: {image_path}\")\n",
    "        continue\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        pixel_values.to(device),\n",
    "        decoder_input_ids=decoder_input_ids.to(device),\n",
    "        max_length=model.decoder.config.max_position_embeddings,\n",
    "        early_stopping=True,\n",
    "        pad_token_id=processor.tokenizer.pad_token_id,\n",
    "        eos_token_id=processor.tokenizer.eos_token_id,\n",
    "        use_cache=True,\n",
    "        num_beams=1,\n",
    "        bad_words_ids=[[processor.tokenizer.unk_token_id]],\n",
    "        return_dict_in_generate=True\n",
    "    )\n",
    "    \n",
    "    sequence = processor.batch_decode(outputs.sequences)[0]\n",
    "    sequence = re.sub(r\"<.*?>\", \"\", sequence, count=1).strip()\n",
    "    predicted_values = processor.token2json(sequence)\n",
    "    \n",
    "    predicted_df.loc[image_link, 'predicted_entity_value'] = predicted_values\n",
    "    \n",
    "    del pixel_values, decoder_input_ids, outputs, sequence, predicted_values\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
